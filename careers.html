<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Careers in AI Safety - COHERENTEYES</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="logo">COHERENTEYES</a>
            <div class="menu-toggle">
                <div class="bar"></div>
                <div class="bar"></div>
                <div class="bar"></div>
            </div>
            <ul class="nav-links">
                <li><a href="index.html">Home</a></li>
                <li><a href="definition.html">What is AI Safety?</a></li>
                <li><a href="perils.html">Perils of AI</a></li>
                <li><a href="careers.html" class="active">Careers</a></li>
                <li><a href="work.html">Our Work</a></li>
            </ul>
        </div>
    </nav>

    <section class="content-section dark-section">
        <div class="container">
            <h2 class="section-title">Careers in AI Safety</h2>
    
            <p class="intro-text">
                Think of AI Safety as the "civil engineering" of the digital age. It is emerging as one of the most interdisciplinary and future-secure fields, blending computer science, law, philosophy, and Policy.
            </p>
            <p class="intro-text">
                The world is waking up to this need. According to the 
                OECD AI Policy Outlook (2024), 
                the demand for AI safety professionals has <strong>more than tripled since 2022</strong> as governments and companies rush to ensure these powerful systems are deployed responsibly.
            </p>
    
            <div class="career-subsection">
                <h3>Why the World Needs More AI Safety Engineers</h3>
                <div class="career-reasons-grid">
                    <div class="career-reason-card">
                        <h4>1. Scale of Impact</h4>
                        <p>AI isn't just in labs anymore; it affects billions of users daily. When a system operates at that scale, even a small error can trigger global consequences.</p>
                    </div>
                    <div class="career-reason-card">
                        <h4>2. It's Getting Complicated</h4>
                        <p>Modern models contain billions of parameters. They are becoming 
                            <span class="term-trigger" tabindex="0">
                                "black boxes"
                                <span class="term-panel">
                                    <strong>Black Box AI:</strong> A system so complex that even its creators cannot explain exactly <em>how</em> it reached a specific decision or answer.
                                </span>
                            </span>, 
                            making it increasingly challenging to verify how they work or interpret their decisions.</p>
                    </div>
                    <div class="career-reason-card">
                        <h4>3. The Law is Catching Up</h4>
                        <p>New regulations like the <strong><a href="https://artificialintelligenceact.eu/the-act/" target="_blank" class="caption-link">EU AI Act</a></strong> and 
                            the <strong><a href="https://bidenwhitehouse.archives.gov/briefing-room/presidential-actions/2025/01/14/executive-order-on-advancing-united-states-leadership-in-artificial-intelligence-infrastructure/" target="_blank" class="caption-link">U.S. Executive Order on AI</a></strong>
                             now require strict risk-management frameworks. This creates immediate institutional demand for safety experts.</p>
                    </div>
                </div>
            </div>
    
            <div class="career-subsection">
                <h3>AI Safety in Southeast Asia</h3>
                <p>Southeast Asia is becoming one of the world's fastest-growing regions for AI adoption. With this growth comes a rising need for safety, governance, and responsible innovation.</p>

                <div class="regional-leaders">
                    <div class="region-card">
                        <h4>Singapore: Regional Leader in AI Governance</h4>
                        <p>
                            Singapore is leading the charge for responsible AI. In 2023, they launched the <strong><a href="https://aiverifyfoundation.sg/downloads/AI_Verify_Primer_Jun-2023.pdf" target="_blank" class="caption-link">AI Verify Foundation (2023)</a></strong>, which acts like a 
                            
                            <span class="term-trigger">
                                "health check" toolkit
                                <span class="term-panel">
                                    <strong>Technical Auditing:</strong> A set of software tools that run tests on AI models to find hidden biases, security holes, or performance errors.
                                </span>
                            </span>
                            
                            for Artificial Intelligence. It allows developers around the world to test their models for fairness and safety before they are released.
                        </p>
                        <p>
                            The government is also putting money behind its vision. The 
                            <strong><a href="https://www.smartnation.gov.sg/initiatives/national-ai-strategy/" target="_blank" class="caption-link">National AI Strategy 2.0</a></strong> 
                            is actively funding training for safety experts, making Singapore the central meeting place for researchers and engineers across Southeast Asia.
                        </p>
                    </div>
                
                    <div class="region-card">
                        <h4>Vietnam: Building Foundations for Safe AI</h4>
                        <p>
                            Vietnam has a bold goal: to become one of the top 4 leaders in ASEAN for AI, a target officially set in its 
                            <strong><a href="https://en.baochinhphu.vn/national-strategy-on-rd-and-application-of-artificial-intelligence-11140663.htm" target="_blank" class="caption-link">National Strategy of AI until 2030</a></strong>. However, this growth is guided by strict safeguards. 
                            The strategy emphasizes "ethical, safe, and responsible development". This is reinforced by the 
                                <strong><a href="https://en.baochinhphu.vn/govt-approves-digital-infrastructure-strategy-11124101010505598.htm" target="_blank" class="caption-link">National Digital Transformation Program to 2030</a></strong>, which establishes the legal framework for data security and trust.
                        </p>
                        <p>
                            Recent evaluations, such as the 
                            <strong><a href="https://www.unesco.org/en/articles/viet-nam-artificial-intelligence-readiness-assessment-report?hub=66932" target="_blank" class="caption-link">Viet Nam: AI Readiness Assessment Report</a></strong>, 
                            highlight the nation's shift toward prioritizing "trust" and 
                            
                            <span class="term-trigger" tabindex="0">
                                "human-centered"
                                <span class="term-panel">
                                    <strong>Human-Centered AI:</strong> Designing systems that prioritize human well-being, rights, and privacy over pure profit or computational efficiency.
                                </span>
                            </span>
                            
                            technology alongside rapid growth.
                        </p>
                        <p>
                            This shift is already happening in classrooms. Top institutions like <strong>VNU, FPT University, and VinAI Research</strong> are teaching the next generation of engineers that "good code" also means
                            
                            <span class="term-trigger" tabindex="0">
                                "ethical code"
                                <span class="term-panel">
                                    <strong>Ethical Coding:</strong> Writing software with safeguards to prevent bias, protect user privacy, and ensure the AI cannot be easily misused for harm.
                                </span>
                            </span>, 
                            
                            integrating safety directly into their training programs.
                        </p>
                    </div>

                    <div class="region-card">
                        <h4>Malaysia and Indonesia: Embedding Trustworthy AI into Policy</h4>
                        <p>Malaysia's 
                            <strong><a href="https://hkifoa.com/wp-content/uploads/2024/12/ai-roadmap-2025-malaysia.pdf" target="_blank" class="caption-link">National AI Roadmap (2021-2025)</a></strong> 
                            identifies responsible and ethical AI as a national pillar, with the Malaysia Digital Economy Corporation (MDEC) coordinating standards and industry partnerships. Indonesia’s 
                            <strong><a href="https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://s899a9742c3d83292.jimcontent.com/download/version/1610650061/module/8284006463/name/AISCI-2020-Indonesia.pdf&ved=2ahUKEwj_1OmXxpWRAxVCh1YBHXnYGvwQFnoECCIQAQ&usg=AOvVaw229hARNNIiZE1C3ipBxV1U" target="_blank" class="caption-link">STRANAS KA (2020)</a></strong> 
                            promotes transparency, accountability, and human-centered AI.
                        </p>
                    </div>
                </div>
                
                <div class="outlook-box">
                    <h4>Outlook</h4>
                    <p>Southeast Asia offers a unique opportunity to build AI safety capacity early, aligning rapid technological growth with robust governance. As countries scale AI deployment in government, healthcare, education, and media, the region will need specialists who combine technical expertise with policy and ethical insight.</p>
                    <p>For emerging engineers and researchers, ASEAN represents not just a fast-growing AI market but a <strong>frontier for safe, trustworthy, and 
                        <span class="term-trigger">
                            "human-centered AI"
                            <span class="term-panel">
                                <strong>Human-Centered AI:</strong> Designing systems that prioritize human well-being, rights, and privacy over pure profit or computational efficiency.
                            </span>
                        </span>
                    </strong></p>
                </div>
            </div>
    
            <div class="career-subsection">
                <h3>4 Pathways to Enter the Field</h3>
                <p style="margin-bottom: 2rem;">You don't just need to be a coder. People enter this field from psychology, economics, policy, and security. Here are four major ways to get involved:</p>
                
                <div class="career-reasons-grid grid-2-col">
                    <div class="career-reason-card">
                        <h4 style="color: #00ff41;">1. Technical Safety Research</h4>
                        <p><strong>The Goal:</strong> Making AI models 
                            <span class="term-trigger" tabindex="0">
                                predictable, robust, and transparent.
                                <span class="term-panel">
                                    <strong>Safe AI Attributes:</strong> Systems that act consistently (predictable), withstand errors or attacks (robust), and allow us to understand their reasoning (transparent).
                                </span>
                            </span>
                        </p>
                        <p style="font-size: 0.9rem; margin-top: 10px; color: #b0b0b0;"><strong>Work involves:</strong></p>
                        <ul style="font-size: 0.9rem; padding-left: 1.2rem; margin-bottom: 10px;">
                            <li>
                                <span class="term-trigger" tabindex="0">
                                    Stress-testing
                                    <span class="term-panel">
                                        <strong>Adversarial Testing:</strong> Intentionally feeding the AI weird, broken, or malicious data to see if it crashes or misbehaves.
                                    </span>
                                </span> 
                                models under unusual conditions </li>

                            <li>Studying 
                                <span class="term-trigger" tabindex="0">
                                    failure modes
                                    <span class="term-panel">
                                        <strong>Edge Cases:</strong> Specific scenarios where the AI is known to fail, such as hallucinating facts, getting tricked by optical illusions, or ignoring safety filters.
                                    </span>
                                </span>  
                                (how and why it breaks) </li>
                                
                            <li>Improving tools to "see inside" the model (
                                <span class="term-trigger" tabindex="0">
                                    interpretability
                                    <span class="term-panel">
                                        <strong>Opening the Black Box:</strong> Techniques to translate the complex internal math of an AI into human-readable logic so we know <em>why</em> it made a decision.
                                    </span>
                                )</li>
                        </ul>
                        <p style="font-size: 0.85rem; border-top: 1px solid rgba(255,255,255,0.1); padding-top: 8px;"><strong>Roles:</strong> Reliability Engineer, ML Safety Researcher</p>
                    </div>
    
                    <div class="career-reason-card">
                        <h4 style="color: #00ff41;">2. AI Governance & Policy</h4>
                        <p><strong>The Goal:</strong> Setting the rules and standards for responsible use.</p>
                        <p style="font-size: 0.9rem; margin-top: 10px; color: #b0b0b0;"><strong>Work involves:</strong></p>
                        <ul style="font-size: 0.9rem; padding-left: 1.2rem; margin-bottom: 10px;">
                            <li>Analyzing policy for governments</li>
                            <li>Auditing systems for compliance</li>
                            <li>Writing safety standards and documentation</li>
                        </ul>
                        <p style="font-size: 0.85rem; border-top: 1px solid rgba(255,255,255,0.1); padding-top: 8px;"><strong>Roles:</strong> AI Policy Analyst, Ethics Advisor</p>
                    </div>
    
                    <div class="career-reason-card">
                        <h4 style="color: #00ff41;">3. Misinformation & Integrity</h4>
                        <p><strong>The Goal:</strong> Preventing the spread of harmful or false information.</p>
                        <p style="font-size: 0.9rem; margin-top: 10px; color: #b0b0b0;"><strong>Work involves:</strong></p>
                        <ul style="font-size: 0.9rem; padding-left: 1.2rem; margin-bottom: 10px;">
                            <li>Building fake news detection models</li>
                            <li>Developing content moderation tools</li>
                            <li>Designing trust and safety systems</li>
                        </ul>
                        <p style="font-size: 0.85rem; border-top: 1px solid rgba(255,255,255,0.1); padding-top: 8px;"><strong>Roles:</strong> Content Safety Engineer, Data Scientist (Integrity)</p>
                    </div>
    
                    <div class="career-reason-card">
                        <h4 style="color: #00ff41;">4. Alignment & Human Feedback</h4>
                        <p><strong>The Goal:</strong> Ensuring AI understands and follows human intentions.</p>
                        <p style="font-size: 0.9rem; margin-top: 10px; color: #b0b0b0;"><strong>Work involves:</strong></p>
                        <ul style="font-size: 0.9rem; padding-left: 1.2rem; margin-bottom: 10px;">
                            <li>Training models with human feedback</li>
                            <li>Studying 
                                <span class="term-trigger" tabindex="0">
                                    generalization and misinterpretation
                                    <span class="term-panel">
                                        <strong>Generalization:</strong> How well AI applies rules to new, unseen situations.<br><br><strong>Misinterpretation:</strong> When AI follows instructions too literally (like a Genie) and violates the user's actual intent.
                                    </span>
                                </span>
                            </li>
                            <li>Designing safe human-AI interactions</li>
                        </ul>
                        <p style="font-size: 0.85rem; border-top: 1px solid rgba(255,255,255,0.1); padding-top: 8px;"><strong>Roles:</strong> Alignment Research Assistant, Human-in-the-Loop Designer</p>
                    </div>
                </div>
            </div>
    
            <div class="cta-section">
                <h2>Ready to Explore Our Work?</h2>
                <p class="cta-description">
                    See how we apply these principles to real-world challenges.
                </p>
                <br>
                <a href="work.html" class="cta-button">View Our Work</a>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <p><strong>COHERENTEYES</strong> — An AI Safety Project</p>
        </div>
    </footer>
    <script src="script.js"></script>
</body>
</html>